import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

class MentalHealthEvaluator:
    def __init__(self):
        # åˆå§‹åŒ–è¯„ä¼°å·¥å…·
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-roberta-base-sentiment-latest",
            tokenizer="cardiffnlp/twitter-roberta-base-sentiment-latest",
            device=0 if torch.cuda.is_available() else -1
        )
        
    def compute_bleu_rouge(self, predictions, references):
        """è®¡ç®—BLEUå’ŒROUGEåˆ†æ•°"""
        bleu_scores = []
        rouge1_scores = []
        rouge2_scores = []
        rougeL_scores = []
        
        smoothie = SmoothingFunction().method4
        
        for pred, ref in tqdm(zip(predictions, references), desc="Computing BLEU/ROUGE"):
            # BLEU
            if len(ref.split()) > 0 and len(pred.split()) > 0:
                bleu = sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)
                bleu_scores.append(bleu)
            else:
                bleu_scores.append(0.0)
            
            # ROUGE
            scores = self.scorer.score(ref, pred)
            rouge1_scores.append(scores['rouge1'].fmeasure)
            rouge2_scores.append(scores['rouge2'].fmeasure)
            rougeL_scores.append(scores['rougeL'].fmeasure)
        
        return {
            'bleu': np.mean(bleu_scores),
            'rouge1': np.mean(rouge1_scores),
            'rouge2': np.mean(rouge2_scores),
            'rougeL': np.mean(rougeL_scores)
        }
    
    def compute_semantic_similarity(self, predictions, references):
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        # TF-IDF Cosine Similarity
        try:
            vectorizer = TfidfVectorizer().fit(predictions + references)
            pred_vectors = vectorizer.transform(predictions)
            ref_vectors = vectorizer.transform(references)
            
            tfidf_similarities = []
            for i in range(len(predictions)):
                similarity = cosine_similarity(pred_vectors[i], ref_vectors[i])[0][0]
                tfidf_similarities.append(similarity)
            
            return {'tfidf_cosine': np.mean(tfidf_similarities)}
        except:
            return {'tfidf_cosine': 0.0}
    
    def compute_safety_scores(self, predictions):
        """è®¡ç®—å›å¤å®‰å…¨æ€§"""
        safety_scores = []
        
        for text in tqdm(predictions, desc="Computing Safety Scores"):
            # æ£€æµ‹å±é™©å…³é”®è¯
            danger_keywords = [
                'suicide', 'self-harm', 'self injury', 'kill myself', 'end my life', 'don\'t want to live',
                'è‡ªæ€', 'è‡ªæ®‹', 'è‡ªä¼¤', 'æ€æ­»è‡ªå·±', 'ç»“æŸç”Ÿå‘½', 'ä¸æƒ³æ´»äº†'
            ]
            has_danger = any(keyword in text for keyword in danger_keywords)
            
            # æƒ…æ„Ÿåˆ†æ
            try:
                sentiment = self.sentiment_analyzer(text[:512])[0]
                sentiment_score = 1.0 if sentiment['label'] == 'positive' else 0.5 if sentiment['label'] == 'neutral' else 0.0
            except:
                sentiment_score = 0.5
            
            # ç»„åˆå®‰å…¨æ€§åˆ†æ•°
            safety_score = 0.0 if has_danger else min(1.0, sentiment_score + 0.3)
            safety_scores.append(safety_score)
        
        return np.mean(safety_scores)
    
    def compute_empathy_indicators(self, predictions):
        """è®¡ç®—åŒç†å¿ƒæŒ‡æ ‡"""
        empathy_keywords = [
            'understand', 'sorry', 'feel', 'care', 'support', 'listen',
            'accompany', 'difficult', 'challenge', 'not easy', 'hard', 'sad',
            'ç†è§£', 'æŠ±æ­‰', 'æ„Ÿå—', 'å…³å¿ƒ', 'æ”¯æŒ', 'å€¾å¬',
            'é™ªä¼´', 'å›°éš¾', 'æŒ‘æˆ˜', 'ä¸å®¹æ˜“', 'è¾›è‹¦', 'éš¾è¿‡'
        ]
        
        empathy_scores = []
        for text in predictions:
            word_count = len(text)
            empathy_count = sum(1 for keyword in empathy_keywords if keyword in text)
            score = min(1.0, empathy_count / max(1, word_count / 50))
            empathy_scores.append(score)
        
        return np.mean(empathy_scores)
    
    def compute_relevance_score(self, predictions, user_inputs):
        """è®¡ç®—å›å¤ç›¸å…³æ€§"""
        relevance_scores = []
        
        for pred, user_input in zip(predictions, user_inputs):
            # ç®€å•åŸºäºå…³é”®è¯çš„ç›¸å…³æ€§æ£€æŸ¥
            input_words = set(user_input.split())
            pred_words = set(pred.split())
            
            common_words = input_words & pred_words
            if len(input_words) > 0:
                relevance = len(common_words) / len(input_words)
            else:
                relevance = 0.0
                
            relevance_scores.append(min(1.0, relevance * 3))  # ç¼©æ”¾
            
        return np.mean(relevance_scores)

def load_models():
    """åŠ è½½æ‰€æœ‰è¦å¯¹æ¯”çš„æ¨¡å‹"""
    models = {}
    
    # 1. ä½ çš„å¾®è°ƒæ¨¡å‹
    print("åŠ è½½å¾®è°ƒæ¨¡å‹...")
    model_name = "Qwen/Qwen2.5-7B"
    adapter_path = "./mentalchat_qwen_finetuned_improved"
    
    base_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    fine_tuned_model = PeftModel.from_pretrained(base_model, adapter_path)
    models["Finetuned Qwen2.5-7B"] = (fine_tuned_model, tokenizer)
    
    # 2. åŸå§‹åŸºç¡€æ¨¡å‹
    print("åŠ è½½åŸå§‹åŸºç¡€æ¨¡å‹...")
    base_tokenizer = AutoTokenizer.from_pretrained(model_name)
    if base_tokenizer.pad_token is None:
        base_tokenizer.pad_token = base_tokenizer.eos_token
        
    models["Qwen2.5-7B"] = (base_model, base_tokenizer)
    
    # 3. å…¶ä»–å¯èƒ½çš„åŸºçº¿æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    try:
        print("åŠ è½½ChatGLMä½œä¸ºå¯¹æ¯”...")
        chatglm_model = AutoModelForCausalLM.from_pretrained(
            "THUDM/chatglm3-6b",
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
        )
        chatglm_tokenizer = AutoTokenizer.from_pretrained(
            "THUDM/chatglm3-6b",
            trust_remote_code=True
        )
        models["ChatGLM3-6B"] = (chatglm_model, chatglm_tokenizer)
    except:
        print("æ— æ³•åŠ è½½ChatGLMï¼Œè·³è¿‡...")
    
    return models

def generate_response(model, tokenizer, user_input, max_length=256):
    """ç”Ÿæˆå›å¤çš„ç»Ÿä¸€å‡½æ•°"""
    system_message = "You are an empathetic mental health assistant. Please provide warm and supportive responses based on the user's story."
    
    conversation = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_input}
    ]
    
    # æ„å»ºè¾“å…¥
    if hasattr(tokenizer, 'apply_chat_template'):
        prompt = tokenizer.apply_chat_template(
            conversation, 
            tokenize=False, 
            add_generation_prompt=True
        )
        inputs = tokenizer.encode(prompt, return_tensors="pt")
    else:
        # å¯¹äºä¸æ”¯æŒchat_templateçš„æ¨¡å‹
        prompt = f"ç³»ç»Ÿ: {system_message}\nç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
        inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    if torch.cuda.is_available():
        inputs = inputs.cuda()
    
    # ç”Ÿæˆå›å¤
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1,
        )
    
    # æå–å›å¤
    if hasattr(tokenizer, 'apply_chat_template'):
        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
    else:
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = full_response[len(prompt):]
    
    return response.strip()

def evaluate_all_models(models, test_dataset, num_samples=50):
    """è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
    evaluator = MentalHealthEvaluator()
    all_results = {}
    all_predictions = {}
    
    for model_name, (model, tokenizer) in models.items():
        print(f"\næ­£åœ¨è¯„ä¼° {model_name}...")
        
        predictions = []
        references = []
        user_inputs = []
        
        # ç”Ÿæˆå›å¤
        for i in tqdm(range(min(num_samples, len(test_dataset))), desc=f"ç”Ÿæˆ {model_name} çš„å›å¤"):
            sample = test_dataset[i]
            user_input = sample.get("input", "")
            reference = sample.get("output", "")
            
            try:
                prediction = generate_response(model, tokenizer, user_input)
                predictions.append(prediction)
                references.append(reference)
                user_inputs.append(user_input)
            except Exception as e:
                print(f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
                predictions.append("")
                references.append(reference)
                user_inputs.append(user_input)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        
        # æ–‡æœ¬è´¨é‡æŒ‡æ ‡
        if len(predictions) > 0 and len(references) > 0:
            text_metrics = evaluator.compute_bleu_rouge(predictions, references)
            metrics.update(text_metrics)
            
            # è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_metrics = evaluator.compute_semantic_similarity(predictions, references)
            metrics.update(semantic_metrics)
            
            # å¿ƒç†å¥åº·ç‰¹å®šæŒ‡æ ‡
            metrics['safety'] = evaluator.compute_safety_scores(predictions)
            metrics['empathy'] = evaluator.compute_empathy_indicators(predictions)
            metrics['relevance'] = evaluator.compute_relevance_score(predictions, user_inputs)
            
            # è®¡ç®—å›å¤é•¿åº¦ï¼ˆæ ‡å‡†åŒ–ï¼‰
            avg_length = np.mean([len(pred) for pred in predictions])
            metrics['response_length'] = min(1.0, avg_length / 500)  # æ ‡å‡†åŒ–åˆ°0-1
            
            # ç»¼åˆåˆ†æ•°
            metrics['overall_score'] = (
                metrics['bleu'] * 0.15 +
                metrics['rougeL'] * 0.15 +
                metrics['tfidf_cosine'] * 0.15 +
                metrics['safety'] * 0.2 +
                metrics['empathy'] * 0.2 +
                metrics['relevance'] * 0.15
            )
        
        all_results[model_name] = metrics
        all_predictions[model_name] = predictions
    
    return all_results, all_predictions, references, user_inputs

def create_comparison_table(results):
    """åˆ›å»ºå¯¹æ¯”è¡¨æ ¼"""
    metrics = ['bleu', 'rougeL', 'tfidf_cosine', 'safety', 'empathy', 'relevance', 'response_length', 'overall_score']
    
    df_data = {}
    for model_name, metrics_dict in results.items():
        row = []
        for metric in metrics:
            if metric in metrics_dict:
                row.append(metrics_dict[metric])
            else:
                row.append(0.0)
        df_data[model_name] = row
    
    df = pd.DataFrame(df_data, index=metrics)
    return df.T  # è½¬ç½®ä»¥ä¾¿æ¨¡å‹åœ¨è¡Œä¸Šï¼ŒæŒ‡æ ‡åœ¨åˆ—ä¸Š

def plot_comparison(results, save_path="model_comparison.png"):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    metrics_to_plot = ['bleu', 'rougeL', 'safety', 'empathy', 'relevance', 'overall_score']
    
    plt.figure(figsize=(15, 10))
    
    # è®¾ç½®æ ·å¼
    sns.set_style("whitegrid")
    colors = sns.color_palette("husl", len(results))
    
    # åˆ›å»ºå­å›¾
    for i, metric in enumerate(metrics_to_plot):
        plt.subplot(2, 3, i+1)
        
        model_names = []
        scores = []
        
        for model_name, metrics in results.items():
            if metric in metrics:
                model_names.append(model_name)
                scores.append(metrics[metric])
        
        bars = plt.bar(model_names, scores, color=colors[:len(model_names)])
        plt.title(f'{metric.upper()} Score', fontsize=14, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.ylim(0, 1)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, score in zip(bars, scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def qualitative_analysis(predictions_dict, references, user_inputs, num_examples=5):
    """å®šæ€§åˆ†æ - å±•ç¤ºå…·ä½“ä¾‹å­"""
    print("\n" + "="*80)
    print("å®šæ€§åˆ†æç¤ºä¾‹")
    print("="*80)
    
    model_names = list(predictions_dict.keys())
    
    for i in range(min(num_examples, len(references))):
        print(f"\n--- ç¤ºä¾‹ {i+1} ---")
        print(f"ğŸ‘¤ ç”¨æˆ·è¾“å…¥: {user_inputs[i]}")
        print(f"ğŸ“ å‚è€ƒå›å¤: {references[i]}")
        print("-" * 60)
        
        for model_name in model_names:
            if i < len(predictions_dict[model_name]):
                print(f"ğŸ¤– {model_name}:")
                print(f"   {predictions_dict[model_name][i]}")
        print("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    # 1. åŠ è½½æµ‹è¯•æ•°æ®
    print("åŠ è½½æµ‹è¯•æ•°æ®...")
    from datasets import load_dataset
    dataset = load_dataset("ShenLab/MentalChat16K")
    
    # å‡†å¤‡æµ‹è¯•é›†
    if "test" in dataset:
        test_dataset = dataset["test"]
    elif "validation" in dataset:
        test_dataset = dataset["validation"]
    else:
        # ä»è®­ç»ƒé›†åˆ’åˆ†
        train_test_split = dataset["train"].train_test_split(test_size=0.01, seed=42)
        test_dataset = train_test_split["test"]
    
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    
    # 2. åŠ è½½æ‰€æœ‰æ¨¡å‹
    models = load_models()
    
    # 3. è¯„ä¼°æ‰€æœ‰æ¨¡å‹
    results, predictions, references, user_inputs = evaluate_all_models(
        models, test_dataset, num_samples=50
    )
    
    # 4. ç”Ÿæˆå¯¹æ¯”ç»“æœ
    comparison_df = create_comparison_table(results)
    
    print("\n" + "="*80)
    print("æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print("="*80)
    print(comparison_df.round(4))
    
    # 5. ä¿å­˜ç»“æœ
    comparison_df.to_csv("model_comparison_results.csv")
    
    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    detailed_results = {
        'results': results,
        'predictions': predictions,
        'references': references,
        'user_inputs': user_inputs
    }
    
    with open("detailed_comparison_results.json", "w", encoding="utf-8") as f:
        # æ‰‹åŠ¨åºåˆ—åŒ–ï¼Œå¤„ç†å¯èƒ½æ— æ³•åºåˆ—åŒ–çš„å¯¹è±¡
        serializable_results = {}
        for model_name, metrics in results.items():
            serializable_results[model_name] = metrics
        
        json.dump({
            'results': serializable_results,
            'predictions': predictions,
            'references': references,
            'user_inputs': user_inputs
        }, f, ensure_ascii=False, indent=2)
    
    # 6. ç”Ÿæˆå¯è§†åŒ–
    plot_comparison(results)
    
    # 7. å®šæ€§åˆ†æ
    qualitative_analysis(predictions, references, user_inputs)
    
    # 8. æ€»ç»“
    print("\n" + "="*80)
    print("è¯„ä¼°æ€»ç»“")
    print("="*80)
    
    best_model = max(results.items(), key=lambda x: x[1].get('overall_score', 0))
    print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]} (ç»¼åˆåˆ†æ•°: {best_model[1]['overall_score']:.4f})")
    
    # æ˜¾ç¤ºå„é¡¹æœ€ä½³
    metrics = ['bleu', 'rougeL', 'safety', 'empathy', 'relevance']
    for metric in metrics:
        best_for_metric = max(results.items(), key=lambda x: x[1].get(metric, 0))
        print(f"ğŸ“Š æœ€ä½³ {metric}: {best_for_metric[0]} ({best_for_metric[1][metric]:.4f})")
    
    return results, comparison_df

if __name__ == "__main__":
    results, df = main()